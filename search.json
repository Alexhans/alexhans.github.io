[
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#pain-points",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#pain-points",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Pain Points",
    "text": "Pain Points\n\nOverwhelming choice: MCPs, Agents\nWe can’t compare what we can’t measure\nCan I trust this to run on its own?\n\n\n\nTranscript coming soon in the form 00:00:00.000 –&gt; 00:00:13.000 My name is Alex - this is a placeholder transcript"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#automation",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#automation",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Automation",
    "text": "Automation\nThis is not a new problem"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#automation-1",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#automation-1",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Automation",
    "text": "Automation\n\nI need to:\n\nBe able to describe what I want\nUnderstand my risk profile"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#integration-unit-tests",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#integration-unit-tests",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Integration / Unit Tests",
    "text": "Integration / Unit Tests"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#integration-unit-tests-1",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#integration-unit-tests-1",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Integration / Unit Tests",
    "text": "Integration / Unit Tests\nf(input) -&gt; desired outcome [✓]"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#integration-unit-tests-2",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#integration-unit-tests-2",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Integration / Unit Tests",
    "text": "Integration / Unit Tests\nf(model, prompt, system prompt, context) -&gt; desired outcome [✓]"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#existing-tools-frameworks",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#existing-tools-frameworks",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Existing Tools & Frameworks",
    "text": "Existing Tools & Frameworks"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#existing-tools-frameworks-1",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#existing-tools-frameworks-1",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Existing Tools & Frameworks",
    "text": "Existing Tools & Frameworks\n\nPromptfoo, Trulens, DeepEval, Ragas, LangFuse, your-custom-built-tool, and many many more"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#how-do-we-write-our-benchmarks",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#how-do-we-write-our-benchmarks",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "How do we write OUR benchmarks",
    "text": "How do we write OUR benchmarks\n(Fast Iteration / Grounded Conversations)"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#how-do-we-write-our-benchmarks-1",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#how-do-we-write-our-benchmarks-1",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "How do we write OUR benchmarks",
    "text": "How do we write OUR benchmarks\n\nIs any dag paused?, [\"dag_a\", \"dag_c\"]\nHas the backfill for X completed?, 'No, currently running tasks is [\"a.task_x\"]'\nHow many SLA breaches have we had in the last 2 weeks?, 4"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#call-to-action-lets-start",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#call-to-action-lets-start",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Call to Action: Let’s start",
    "text": "Call to Action: Let’s start\n(“Reproducibility” First)"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#whos-interested",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#whos-interested",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Who’s interested?",
    "text": "Who’s interested?\n(MCP pioneers, I’m looking at you)"
  },
  {
    "objectID": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#thank-you",
    "href": "talks/airflow-summit/toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html#thank-you",
    "title": "Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "Thank you!",
    "text": "Thank you!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’ve been developing software professionally from 2008 so I’ve seen a thing or two, writing roguelikes with GCC 4.2, dealing with Visual C++ 6.0’s non standard quirks, writing triangles in OpenGL’s “Red Book”, writing fiscal printer firmware, creating multi country products, working in truly big data in Big Company (TM) and realizing that more often than not, the problem is not technical in nature.\nI’ve always found myself repeating insights, whether learned from others or my own experiences and realized I don’t always put them into text in the companies I work for and definitely not expose them in written form to the outside world.\nI should do that more. This is me getting into the habit of doing that more."
  },
  {
    "objectID": "posts/series/evals/measure-first-optimize-last.html",
    "href": "posts/series/evals/measure-first-optimize-last.html",
    "title": "Measure First, Optimize Last: My Approach to AI Evals",
    "section": "",
    "text": "If you can’t measure it, you’re guessing. Here’s how I think about evals, practical examples at ai-evals.io."
  },
  {
    "objectID": "posts/series/evals/measure-first-optimize-last.html#footnotes",
    "href": "posts/series/evals/measure-first-optimize-last.html#footnotes",
    "title": "Measure First, Optimize Last: My Approach to AI Evals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBlast Radius: refers to the extent of system, data, or user impact caused by a component failure, security breach, or faulty code deployment. See an extreme example in the Falcon Sensor 2024 crash.↩︎\nTDD: Test Driven Development: The act of driving your code thinking intentionally about testability and what tests expose the behaviour you want. Dogmatically it can seem slow and unhelpful but thinking about testability and adding tests to catch prevent bugs from recurring are very useful practices to have. Otherwise, overbuilding is very easy.↩︎"
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html",
    "href": "posts/series/zeroops/no-news-is-good-news.html",
    "title": "No News Is Good News",
    "section": "",
    "text": "Do not check whether things are fine."
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html#the-rule",
    "href": "posts/series/zeroops/no-news-is-good-news.html#the-rule",
    "title": "No News Is Good News",
    "section": "",
    "text": "Do not check whether things are fine."
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html#pain-point",
    "href": "posts/series/zeroops/no-news-is-good-news.html#pain-point",
    "title": "No News Is Good News",
    "section": "Pain Point",
    "text": "Pain Point\nManually checking systems to confirm they are “okay” creates unnecessary cognitive load.\nDashboards and queues become reassurance rituals: they consume time and attention without changing outcomes.\nIf something is broken, you should be told."
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html#do",
    "href": "posts/series/zeroops/no-news-is-good-news.html#do",
    "title": "No News Is Good News",
    "section": "Do",
    "text": "Do\nFor anything that might require action, ensure there is an automated signal.\n\nThe signal should reach the people who can meaningfully act on it.\nIt does not need to prescribe the action.\nOver time, actions may be formalised (runbooks, automation), but that is secondary.\n\nIf something requires attention, it should create noise. If it does not, it should remain silent."
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html#do-not",
    "href": "posts/series/zeroops/no-news-is-good-news.html#do-not",
    "title": "No News Is Good News",
    "section": "Do Not",
    "text": "Do Not\n\nCreate mechanisms to confirm system health.\nRegularly inspect dashboards “just to be sure”.\nRely on manual checks for reassurance.\n\nSilence is expected when coverage is adequate."
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html#scope",
    "href": "posts/series/zeroops/no-news-is-good-news.html#scope",
    "title": "No News Is Good News",
    "section": "Scope",
    "text": "Scope\nThis applies to operational, actionable failures: things that are broken now and require attention.\nThis does not cover:\n\nslow degradation\ntrend monitoring\npreemptive or exploratory analysis"
  },
  {
    "objectID": "posts/series/zeroops/no-news-is-good-news.html#analogy",
    "href": "posts/series/zeroops/no-news-is-good-news.html#analogy",
    "title": "No News Is Good News",
    "section": "Analogy",
    "text": "Analogy\nThink of a perfect assistant: they interrupt you only when there is something you can act on. If they do not interrupt you, you can assume everything is fine."
  },
  {
    "objectID": "posts/talk.toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html",
    "href": "posts/talk.toward-a-shared-vision-of-llm-evals-in-airflow-ecosystem.html",
    "title": "Talks: Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem",
    "section": "",
    "text": "Abstract\nAs LLM tools and agents emerge in the Airflow community, whether as plugins, MCP servers, or embedded agents, we lack a consistent way to benchmark across implementations and across versions of the same solution. This lightning talk highlights the need of an agreed-upon evaluation mechanism that enables us to measure, compare, and reproduce results when working with GenAI solutions in relation to Airflow. I’ll share what such mechanism could look like in practice. If you care about building trustworthy, testable GenAI systems (that could eventually fit into CI/CD workflows) and want to able to have grounded discussions when developing in this space, let’s lay the groundwork to test and compare our tools meaningfully.\n\n\nSlides and Transcript\n\nToward a Shared Vision for LLM Evaluation in the Airflow Ecosystem\nTranscript in VTT coming before 2026-02-01"
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html",
    "href": "posts/wsl-pip-hangs-ipv6.html",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "",
    "text": "pip install hangs in WSL with no useful error, often after it starts fetching from files.pythonhosted.org."
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#pain-point",
    "href": "posts/wsl-pip-hangs-ipv6.html#pain-point",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "",
    "text": "pip install hangs in WSL with no useful error, often after it starts fetching from files.pythonhosted.org."
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#the-rule",
    "href": "posts/wsl-pip-hangs-ipv6.html#the-rule",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "The Rule",
    "text": "The Rule\nIf DNS/connection to files.pythonhosted.org hangs but pypi.org works, suspect IPv6 preference + broken IPv6 routing."
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#minimal-diagnosis",
    "href": "posts/wsl-pip-hangs-ipv6.html#minimal-diagnosis",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "Minimal Diagnosis",
    "text": "Minimal Diagnosis\npython -c \"import urllib.request; print(urllib.request.urlopen('https://pypi.org/simple/').status)\"\n# expected: 200\ngetent hosts pypi.org\n# returns quickly\ngetent hosts files.pythonhosted.org\n# may hang\nIf files.pythonhosted.org hangs, pip will hang. That host is where wheels and sdists are served from."
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#fix",
    "href": "posts/wsl-pip-hangs-ipv6.html#fix",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "Fix",
    "text": "Fix\nPrefer IPv4 for address selection using gai.conf:\nsudo tee /etc/gai.conf &gt;/dev/null &lt;&lt;'EOF'\nprecedence ::ffff:0:0/96  100\nEOF\nThis does not disable IPv6. It changes the precedence so IPv4 is tried first."
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#verify",
    "href": "posts/wsl-pip-hangs-ipv6.html#verify",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "Verify",
    "text": "Verify\ngetent hosts files.pythonhosted.org\n# should return immediately\nThen retry:\npip install ipython"
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#revert",
    "href": "posts/wsl-pip-hangs-ipv6.html#revert",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "Revert",
    "text": "Revert\nsudo tee /etc/gai.conf &gt;/dev/null &lt;&lt;'EOF'\n# empty override: use glibc defaults\nEOF"
  },
  {
    "objectID": "posts/wsl-pip-hangs-ipv6.html#notes",
    "href": "posts/wsl-pip-hangs-ipv6.html#notes",
    "title": "Fix: pip hangs in WSL (IPv6 / gai.conf)",
    "section": "Notes",
    "text": "Notes\nIf you see multiple stuck installs, clear them before retrying:\npkill -f \"python -u -m pip install\" || true"
  },
  {
    "objectID": "posts/notifications-for-command-line-tasks.html",
    "href": "posts/notifications-for-command-line-tasks.html",
    "title": "Get notifications in ubuntu when command line tasks end",
    "section": "",
    "text": "Often, when working in the terminal, you’ll find yourself running a command that takes a non-trivial amount of time and you don’t want to just stare at the screen until it finishes.\nSo you switch tabs/windows and do something else in the meantime. Problem is, when is the other task finished? You don’t want to waste time checking too often nor too late…\nSo what you want is a notification. One that lets you carry on merrily until the original command is actually finished.\nIt turns out that many .bashrc files come with an alias called alert and, some SO answers even improve upon it."
  },
  {
    "objectID": "posts/notifications-for-command-line-tasks.html#intro",
    "href": "posts/notifications-for-command-line-tasks.html#intro",
    "title": "Get notifications in ubuntu when command line tasks end",
    "section": "",
    "text": "Often, when working in the terminal, you’ll find yourself running a command that takes a non-trivial amount of time and you don’t want to just stare at the screen until it finishes.\nSo you switch tabs/windows and do something else in the meantime. Problem is, when is the other task finished? You don’t want to waste time checking too often nor too late…\nSo what you want is a notification. One that lets you carry on merrily until the original command is actually finished.\nIt turns out that many .bashrc files come with an alias called alert and, some SO answers even improve upon it."
  },
  {
    "objectID": "posts/notifications-for-command-line-tasks.html#desktop-notifications-with-notify-send",
    "href": "posts/notifications-for-command-line-tasks.html#desktop-notifications-with-notify-send",
    "title": "Get notifications in ubuntu when command line tasks end",
    "section": "Desktop notifications with notify-send",
    "text": "Desktop notifications with notify-send\nHere’s the one I’m using lately and has served me well:\n# Add an \"alert\" alias for long running commands.  Use like so:\n#   sleep 10; alert\nalias alert='notify-send --urgency=low -i \"$([ $? = 0 ] && echo terminal || echo error)\"  \n\"$(history|tail -n1|sed -e '\\''s/^\\s*[0-9]\\+\\s*//;s/[;&|]\\s*alert$//'\\'')\"'\nAs the comment says, using it is just a matter of writing the command you want, a semi-colon and the alias alert (Remember that semi-colon ; means execute after the previous command is finished, no matter the return code, unlike && which only executes the next command if return code is 0 (success).\nSo if you’re compiling and running tests in a project you could just do:\nmake test; alert\nand you’ll get notified whenever make test ends.\nBut what if you decided running that lengthy task is a good moment to step away from your computer and take a coffee break or talk with a coworker? How will you know when it’s done if you’re not in front of the computer to see the desktop notification?"
  },
  {
    "objectID": "posts/notifications-for-command-line-tasks.html#email-notifications",
    "href": "posts/notifications-for-command-line-tasks.html#email-notifications",
    "title": "Get notifications in ubuntu when command line tasks end",
    "section": "Email notifications",
    "text": "Email notifications\nThat’s when email comes in handy. You just gotta take your phone with you and have access to an SMTP server.\n$ sudo apt install mailutils\nThe logic is the same as before, once the command is done, execute the “alert”.\nIf you want to do it in python, here’s a simple way to go about it.\nJust make sure it doesn’t go to SPAM.\nCheers\n\nWas this helpful? Do you do it another way? All comments are welcome!"
  },
  {
    "objectID": "posts/using-dbt-to-accelerate-science.html",
    "href": "posts/using-dbt-to-accelerate-science.html",
    "title": "Using Data Build Tool (dbt) to Accelerate & Scale Science",
    "section": "",
    "text": "This post is part of a series: “Factory of Domain Experts”:"
  },
  {
    "objectID": "posts/using-dbt-to-accelerate-science.html#what-problem-are-we-solving",
    "href": "posts/using-dbt-to-accelerate-science.html#what-problem-are-we-solving",
    "title": "Using Data Build Tool (dbt) to Accelerate & Scale Science",
    "section": "What problem are we solving?",
    "text": "What problem are we solving?\n“When can we launch this?” is a recurring question in cross-functional teams, and the answer is often “ask the engineers”. But is that really necessary? Do scientists need to build and then hand off to engineers to rewrite code for scalability or reliability?\nI challenged this pattern since I wanted to scale without having to grow engineering headcount, and empower our scientists to deliver more impact independently. The original handover approach introduced delays, estimation misses, integration surprises, and iteration overhead.\n\nWhat we wanted to achieve:\n\nScale our team: Grow scientific output independent of engineering capacity.\nIterate in parallel, not sequentially: Team members collaborate building and integrating simultaneously, without waiting periods or handovers.\nShare easily reproducible code: Produce reproducible code and data that makes cross-team collaboration easy and transparent.\n\nInstead of building custom solutions, we used a small set of industry-standard tools, dbt (Data Build Tool) with SQL, Apache Airflow (using astronomer-cosmos), and Git, to create a simple system. Scientists now develop close to the domain, and their work is automatically orchestrated and deployed without engineers needing to rewrite or manage the code. There’s no custom Graphical User Interface (GUI) or platform, just clear conventions, smart defaults, and infrastructure-as-code. Engineers focus on building reusable capabilities while scientists focus on science and business logic."
  },
  {
    "objectID": "posts/using-dbt-to-accelerate-science.html#how-dbt-solves-these-problems",
    "href": "posts/using-dbt-to-accelerate-science.html#how-dbt-solves-these-problems",
    "title": "Using Data Build Tool (dbt) to Accelerate & Scale Science",
    "section": "How dbt solves these problems",
    "text": "How dbt solves these problems\nData Build Tool (dbt) enables engineers and scientists alike to transform data using software engineering best practices. Crucially, there are no tradeoffs between scrappy exploration and production-ready code, the same code serves both purposes:\n\nProduction-ready from day one: The code scientists write IS the production code. No handovers, no rewrites, no “let me translate this for production.” Your development SQL becomes the scheduled pipeline automatically.\nCollaboration and early integration: Since both engineers and scientists can run the same dbt code, collaboration happens naturally from day one, fostering cross-domain learning and surfacing integration or reproducibility issues early, reducing project risk.\nSimple workflows that scale: A simple dbt run -s \"model_name+\" runs your model and all dependencies. The same code that works for individual data exploration works for production scheduling.\nModularity without orchestration headaches: dbt forces you to break apart monolithic SQL into focused models, but handles all the dependency management automatically, so you get the benefits of clean, debuggable code without the cognitive overhead of managing execution order.\nAutomatic lineage and documentation: dbt generates interactive dependency graphs showing how your models connect. Schema documentation automatically appears in the warehouse tables.\nBuilt-in quality controls: Define data tests that run automatically.\nBuilt for integration and extensibility: dbt integrates seamlessly with our existing AWS stack (Athena, Glue, Iceberg), internal services and datalakes and industry standard tools.\nCompliance and governance: Data policies can be built into packages, ensuring compliance and empowering your users to make the right tradeoffs around data handling."
  },
  {
    "objectID": "posts/using-dbt-to-accelerate-science.html#impact",
    "href": "posts/using-dbt-to-accelerate-science.html#impact",
    "title": "Using Data Build Tool (dbt) to Accelerate & Scale Science",
    "section": "Impact",
    "text": "Impact\nOur approach enabled delivery of multiple high-impact scientist-led projects that would have otherwise been delayed or blocked due to engineering constraints. Peer teams adopted or expressed desires to adopt it, when they had a chance to work with us and experiment the productivity speed ups, in different dimensions."
  },
  {
    "objectID": "posts/slack/stop-reformatting-markdown-when-pasting-into-slack.html",
    "href": "posts/slack/stop-reformatting-markdown-when-pasting-into-slack.html",
    "title": "Stop Reformatting Markdown When Pasting into Slack",
    "section": "",
    "text": "Slack only pastes rich formatting when the clipboard advertises text/html, otherwise it treats everything as plain text.\nIf my file sample.md looks like this:\n:robot_face: Tech updates :robot_face:\n\n# Some Title\n\n- **Project**: Did x in [google](https://google.com).\n  - aaa\n  - bbb\n      - ccc \n  - ddd\n\n# Another title\n\n- Another launch\n  - details\nCompare pasting directly on the left and what we want on the right.\n\n\n\nSide by Side comparison between markdown in plain text and the slack rich formatted version"
  },
  {
    "objectID": "posts/slack/stop-reformatting-markdown-when-pasting-into-slack.html#how-to-build-latest-xclip-from-source-in-ubuntu",
    "href": "posts/slack/stop-reformatting-markdown-when-pasting-into-slack.html#how-to-build-latest-xclip-from-source-in-ubuntu",
    "title": "Stop Reformatting Markdown When Pasting into Slack",
    "section": "How to build latest xclip from source in Ubuntu",
    "text": "How to build latest xclip from source in Ubuntu\nsudo apt install autoconf automake libtool libxmu-dev\ngit clone https://github.com/astrand/xclip\ncd xclip\nautoreconf -fi\n./configure --prefix=/usr/local\nmake\nsudo make install"
  },
  {
    "objectID": "posts/slack/stop-reformatting-markdown-when-pasting-into-slack.html#wsl-and-powershell-are-different-beasts",
    "href": "posts/slack/stop-reformatting-markdown-when-pasting-into-slack.html#wsl-and-powershell-are-different-beasts",
    "title": "Stop Reformatting Markdown When Pasting into Slack",
    "section": "WSL and powershell are different beasts",
    "text": "WSL and powershell are different beasts\nThis won’t work on WSL and slack as-is. You likely need to do it from powershell using a third-party program\nWSL cannot directly populate the Windows clipboard with rich HTML in a way Slack accepts; an intermediate Windows application re-copies the content with additional clipboard formats.\n\nPowershell 5\n\nGet-Content out.html -Raw | Set-Clipboard -AsHtml\n\nOpen LibreOffice Writer or any GUI and paste.\nSelect that and copy\nPaste into slack\n\n\nFormat HTML won’t be added to Powershell 7\nPeople are not happy"
  },
  {
    "objectID": "posts/series/zeroops/merge-and-forget.html",
    "href": "posts/series/zeroops/merge-and-forget.html",
    "title": "Merge and Forget",
    "section": "",
    "text": "After your change is approved and enters the delivery pipeline, you should be able to forget it.\nNo following it through pipelines. No watching for when it lands. No manual checking for failure states."
  },
  {
    "objectID": "posts/series/zeroops/merge-and-forget.html#the-rule",
    "href": "posts/series/zeroops/merge-and-forget.html#the-rule",
    "title": "Merge and Forget",
    "section": "",
    "text": "After your change is approved and enters the delivery pipeline, you should be able to forget it.\nNo following it through pipelines. No watching for when it lands. No manual checking for failure states."
  },
  {
    "objectID": "posts/series/zeroops/merge-and-forget.html#pain-point",
    "href": "posts/series/zeroops/merge-and-forget.html#pain-point",
    "title": "Merge and Forget",
    "section": "Pain Point",
    "text": "Pain Point\nTracking deployments “just in case” creates unnecessary cognitive load.\nIt turns delivery into a background worry: tabs left open, dashboards checked, attention fragmented.\nIf something requires attention, you should be told."
  },
  {
    "objectID": "posts/series/zeroops/merge-and-forget.html#do",
    "href": "posts/series/zeroops/merge-and-forget.html#do",
    "title": "Merge and Forget",
    "section": "Do",
    "text": "Do\nTreat delivery as a system property:\n\nPush “surprises” left: run fast, automated cross-system checks (e.g. integration tests) before merge, at code-review time, to minimise post-merge failures.\nYou should get a signal if something is blocked or broken.\nYou should not need manual reassurance.\nWhen the system is healthy, silence is expected.\n\n(see No News Is Good News)."
  },
  {
    "objectID": "posts/series/zeroops/merge-and-forget.html#do-not",
    "href": "posts/series/zeroops/merge-and-forget.html#do-not",
    "title": "Merge and Forget",
    "section": "Do Not",
    "text": "Do Not\n\nFollow a deploy through the pipeline to feel safe.\nKeep checking “did it land yet?”\nWatch logs/dashboards after merge for reassurance."
  },
  {
    "objectID": "posts/series/zeroops/merge-and-forget.html#scope",
    "href": "posts/series/zeroops/merge-and-forget.html#scope",
    "title": "Merge and Forget",
    "section": "Scope",
    "text": "Scope\nThis is for routine, continuous delivery of typical changes.\nThis does not cover:\n\nmajor migrations\none-way / high-blast-radius changes\ncommunicating expected delivery times (ETAs)"
  },
  {
    "objectID": "posts/series/evals/automate-audits.html",
    "href": "posts/series/evals/automate-audits.html",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "",
    "text": "We keep asking questions we should already know the answer to. And we usually ask them when a decision depends on it.\nSometimes we guess. Sometimes we do a one-off investigation. Sometimes we shrug and move on.\nWhat if you could get the value of a high-quality audit all the time?"
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#the-idea",
    "href": "posts/series/evals/automate-audits.html#the-idea",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "The idea",
    "text": "The idea\nInstead of running audits occasionally, automate the audit itself.\nAn audit is just a set of questions.\nIf you make those questions explicit, and make them answerable repeatedly, the audit stops being a one-off activity and becomes something you can run continuously."
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#the-method",
    "href": "posts/series/evals/automate-audits.html#the-method",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "The method",
    "text": "The method\nStart from pain points you already feel, or decisions you struggle to make.\nFrom each pain point, write the questions that would help you address it.\nThose questions imply dimensions (what you want coverage over) and, over time, a set of entities that describe your world.\nDon’t try to be complete. Just describe enough of your world to support the questions you care about.\nFor example, “we don’t know if we can reproduce the data of our projects” is a pain point that may prompt the questions:\n\nWhich projects have code packages?\nWhich code packages have a README with reproducibility steps?\nWhich README file instructions actually work?\n\nThe first two should be relatively trivial to check (in this GenAI world) and you can decide how much value the third one gets but the important point is that you’ve introduced what you know and surfaced what you currently can answer in case it’s worth it later.\nThe dimensions in our example are Code Packages and Projects.\nThese are actionable and describe different states of knowledge and reproducibility readiness.\n\n\n\n\n\n\nNote\n\n\n\nOther questions will likely arise from these such as “Which projects are ongoing? Which code packages belong to which projects?”"
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#the-entities-in-your-world",
    "href": "posts/series/evals/automate-audits.html#the-entities-in-your-world",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "The entities in your world",
    "text": "The entities in your world\nAfter writing a few questions, you can jump to a whiteboard and try to describe a lot of the entities in your world to brainstorm about what you own, what you interact with and how things cluster together. The entities are the targets of the questions, and dimensions allow you to define the coverage. Example: “We have a total of 12 projects: 5 have READMEs with reproducibility steps, 2 don’t have READMEs, 5 have no packages.”\nIt’s up to you whether you want to immediately define actions to take or treat it as a helpful data point for others to decide. The point is you stop spending time re-answering ad hoc questions."
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#the-forcing-function",
    "href": "posts/series/evals/automate-audits.html#the-forcing-function",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "The forcing function",
    "text": "The forcing function\nOnce you have questions, you write evals, checks that verify you can answer them. (See promptfoo’s documentation for one way to implement them.)\nAn eval is simply:\n\nIf I ask this question, I expect to get this kind of answer.\n\nWriting the eval is where the value appears.\nThe moment you write it, you’re forced to confront whether you can actually answer the question at all.\n\nIf you can, the eval is straightforward.\nIf you can’t, you’ve discovered something you thought you knew but didn’t.\nIf the answer is subjective, that gap becomes explicit.\n\nYou don’t need perfect answers. You need to know which evals pass and which ones reveal that you never had a real answer.\nThat alone is valuable.\n\n\n\n\n\n\nNote\n\n\n\nWhile writing evals in a form that doesn’t break as things change requires some practice, it’s not your primary concern when starting. It’s fine to be explicit and baseline."
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#what-you-get",
    "href": "posts/series/evals/automate-audits.html#what-you-get",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "What you get",
    "text": "What you get\nOnce questions have evals, answering them becomes an implementation detail. With MCPs (Model Context Protocol servers), Playwright, etc. Programmatic access is easier than ever, the hard part isn’t answering questions, it’s knowing which ones to ask and systematizing what “good” looks like.\nOver time, you start to see where your questions apply, which are too vague, which are easier than expected, and where your description of the world is still thin.\nYou don’t need to go all in. Start with a few questions and a small report that grows over time.\nAt that point, you’re no longer redoing one-off audits.\nYou’re running a continuous audit whose scope is defined by the questions you care about."
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#why-this-is-low-risk",
    "href": "posts/series/evals/automate-audits.html#why-this-is-low-risk",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "Why this is low-risk",
    "text": "Why this is low-risk\n\nYou don’t need to know everything upfront.\nYou don’t need to define “good” everywhere.\nYou don’t need a complete model of your world.\n\nYou just need to acknowledge you may not know the answer for questions you’ve not explicitly tried to answer (quite similar to feeling pain when doing unit testing while being unsure of what you want)\nEven unanswered questions are useful. They tell you what’s unclear, subjective, or not worth investing in."
  },
  {
    "objectID": "posts/series/evals/automate-audits.html#closing",
    "href": "posts/series/evals/automate-audits.html#closing",
    "title": "Get the Value of a High-Quality Audit, All the Time",
    "section": "Closing",
    "text": "Closing\nYou don’t need to audit everything.\nYou just need to stop rediscovering the same answers, and the same unknowns, over and over again.\nStart with a few questions. Let the audit grow.\nIf it doesn’t help, stop.\nThe cost is small. The insight compounds."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "",
    "text": "GenAI is non-deterministic and can fail or produce different results for the same input.\nA typical prompt-to-action flow involves many LLM calls. Each call is a chance for the model to misinterpret, hallucinate, or produce an unusable output.\nThe question isn’t if errors happen. It’s what happens when they do, and how many opportunities you give them to cascade."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html#how-bad-does-it-get",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html#how-bad-does-it-get",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "How bad does it get?",
    "text": "How bad does it get?\nConsider a simple case with just 4 steps (note: this is illustrative, consider how your system might have 10, 20, or more LLM calls):\n\nStep 1: 95% chance of correct\nStep 2: 95% chance of correct\n\nStep 3: 95% chance of correct\nStep 4: 95% chance of correct\n\nEnd-to-end: ~81% chance everything is correct.\nNow compare:\n\nStep 1 (LLM): user intent → structured call (95%)\nStep 2 (deterministic tool): execute (98%)\nStep 3 (deterministic validation): parse + check (97%)\nStep 4 (LLM): result → response (96%)\n\nEnd-to-end: ~87%\n\nSame model. Different architecture. 6+ point improvement."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html#two-high-leverage-approaches",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html#two-high-leverage-approaches",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "Two high-leverage approaches",
    "text": "Two high-leverage approaches\n\nRemove 1 or many GenAI steps entirely, fewer chances to fail\nReplace GenAI steps with deterministic ones, lower error rate per step\n\n\n\n\n\n\n\nNote\n\n\n\nThese aren’t the only ways to reduce error (e.g. consensus systems, retries, etc) but the fundamentals expressed would apply everywhere, no matter whether it’s one agent or a swarm."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html#what-makes-deterministic-steps-different",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html#what-makes-deterministic-steps-different",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "What makes deterministic steps different",
    "text": "What makes deterministic steps different\nDeterministic steps still fail, but the failure characteristics differ from LLM failures:\n\nBounded: failures come from a finite set of causes (parse error, timeout, missing field), not open-ended misinterpretation\nRepeatable: same input, same failure: you can reproduce and fix it\nNon-semantic: a crashed process doesn’t convince the next step that “actually the user meant X”\n\n\n\n\n\n\n\nNote\n\n\n\nThis doesn’t mean deterministic = reliable. It means when things break, they break in less subtle ways and there’s a lot of software engineering history behind their robustness."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html#the-design-pattern",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html#the-design-pattern",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "The design pattern",
    "text": "The design pattern\nLLMs do many hard parts (interpreting intent, choosing tools, dealing with syntax, reasoning through results, deciding what comes next).\nIn a simplified flow, the model might:\n\nReceive user intent (natural language)\nDecide which tool to call and with what parameters\nReceive structured output from the tool\nDecide: done, or call another tool?\nRepeat until ready to respond\nTranslate the final result back to the user\n\nA lot of reasoning and orchestration happens there and the point isn’t to limit that but to give it good building blocks.\nA human user is more effective with better building blocks (e.g. well designed libraries or cli tools) and so is an LLM.\nWhat you want are building blocks that are:\n\nreusable\ncomposable\nwell tested\neasy to change and maintain\ncost effective\n\nAnd a model that acts as a translation layer, not the tool running all the logic."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html#practical-recommendations",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html#practical-recommendations",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "Practical recommendations",
    "text": "Practical recommendations\n\nIdentify the deterministic core\nIf you are writing a Claude skill and a step can be expressed as code, ask yourself why you’re not expressing it as code.\nThe tradeoff is real:\nLeaving logic in prose means:\n\nHigher error rate at runtime\nRelying on evals instead of unit tests (if you don’t know what one or either of these are, then you’re definitely safer in the frozen code world)\nPaying the cost and error on every execution\nYes, it might improve as models improve, but you’re paying for that uncertainty every time\n\nMoving logic to code means:\n\nLower error rate (deterministic execution)\nUnit testable\nCheaper to run\nStill easy to write with LLMs, have the model generate the code once instead of regenerating the logic from prose every time\nYou can still ask LLMs to review or improve the code later if you want\n\nThe second option gives you confidence that things actually work. The first option defers that confidence in exchange for alleged convenience.\nIf the LLM can write code for you, why have it translate markdown to logic on every run? Make the translation once, freeze it as code, and test it properly.\n\n\nForce structure at boundaries\nDon’t pass prose between steps. Use formats that are easy to serialize and deserialize, like JSON/YAML with schemas you can validate against.\nStructure lets you validate, detect errors and attempt course correction or fail fast, diff, log, and evaluate deterministically.\n\n\n\n\n\n\nNote\n\n\n\nThis will even save you money, time and compute resources by not having to use LLM as a Judge for assertion in your evals.\n\n\n\n\nTest the building blocks\nWrite unit tests and integration tests for the core building blocks — same as you would’ve done before LLMs."
  },
  {
    "objectID": "posts/series/evals/error-compounding-genai-systems-approach.html#closing",
    "href": "posts/series/evals/error-compounding-genai-systems-approach.html#closing",
    "title": "Reducing Error Compounding in GenAI Systems",
    "section": "Closing",
    "text": "Closing\nThis isn’t about distrusting models, it’s about giving them good building blocks to use.\nUse GenAI to translate intent. Use the building blocks to execute. Keep errors where you can measure them.\nThat is how automation becomes something you can trust instead of manually testing once and being hopeful."
  },
  {
    "objectID": "posts/accept-self-signed-cert-git-https.html",
    "href": "posts/accept-self-signed-cert-git-https.html",
    "title": "Accept a self-signed certificate with git",
    "section": "",
    "text": "Some time ago I came into an issue where people served git repositories in a local network using apache but used a self-signed certificate for the server.\nEveryone was already trained to add the exception in their browsers to access HTML content but what happened when it came to source code control?"
  },
  {
    "objectID": "posts/accept-self-signed-cert-git-https.html#intro",
    "href": "posts/accept-self-signed-cert-git-https.html#intro",
    "title": "Accept a self-signed certificate with git",
    "section": "",
    "text": "Some time ago I came into an issue where people served git repositories in a local network using apache but used a self-signed certificate for the server.\nEveryone was already trained to add the exception in their browsers to access HTML content but what happened when it came to source code control?"
  },
  {
    "objectID": "posts/accept-self-signed-cert-git-https.html#the-problem",
    "href": "posts/accept-self-signed-cert-git-https.html#the-problem",
    "title": "Accept a self-signed certificate with git",
    "section": "The Problem",
    "text": "The Problem\nIt turns out Subversion (SVN) presented no issue since it prompted the user to accept the new server key just once and then didn’t pester them again but git was another story. Git tried to verify that the cert was signed by a proper authority and couldn’t.\nuser@user-linux:git$ git clone https://user@dev-server-01/git/repo_name.git \nCloning into 'repo_name'...\nfatal: unable to access 'https://user@dev-server-01/git/repo_name.git/': server certificate verification failed. \nCAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none"
  },
  {
    "objectID": "posts/accept-self-signed-cert-git-https.html#the-solution",
    "href": "posts/accept-self-signed-cert-git-https.html#the-solution",
    "title": "Accept a self-signed certificate with git",
    "section": "The Solution",
    "text": "The Solution\nAfter some googling I came across suggestions to disable SSL verification with git config http.sslVerify \"false\" but that looked like it could induce some bad habits and it actually wouldn’t prevent tampering if, for instance, the user was pointed elsewhere instead of the proper original server.\nThat’s when Stack Overflow came into play and I found about this neat solution where you can associate a hostname with a given certificate that you store locally.\nSteps:\n1- Download the self signed certificate from the server and store it somewhere like /etc/ssl/certs\n/etc/ssl/certs/ssl-cert-dev-01.pem\n/etc/ssl/certs/ssl-cert-dev-02.pem\n2- Modify your git config (globally or per-repository) to associate hosts with certs:\n(From git config --help)\n\nhttp.sslCAInfo\n    File containing the certificates to verify the peer with when fetching or pushing over HTTPS. \n    Can be overridden by the GIT_SSL_CAINFO environment variable.\nIn this case we’re going to do it globally by modifying ~/.gitconfig\n[http \"https://dev-server-01:/\"]\n    sslCAInfo = /etc/ssl/certs/ssl-cert-dev-01.pem\n\n[http \"https://dev-server-02\"]\n    sslCAInfo = /etc/ssl/certs/ssl-cert-dev-02.pem\nOr you can do it with the command line:\n$ git config --global http.\"https://dev-server-01/\".sslCAInfo /etc/ssl/certs/ssl-cert-dev-01.pem\n$ git config --global http.\"https://dev-server-02/\".sslCAInfo /etc/ssl/certs/ssl-cert-dev-02.pem\nOf course, this breaks the flow of those who were using HTTP and the IP address directly since you need the same name that appears in the certificate. That’s the one con I can think of and, if your users where not in the habit of doing so, you’ll better start getting them used to it.\nCheers\n\nWas this helpful? Do you do it another way? All comments are welcome!"
  },
  {
    "objectID": "posts/aider-with-open-router.html",
    "href": "posts/aider-with-open-router.html",
    "title": "Use aider for free with your local LLMs or cheaply with OpenRouter",
    "section": "",
    "text": "Many people use LLM (Large Language Models) services to code at work but don’t necessarily see a path to use them at home on a budget.\nHere are two quick recipes: one for a fully local, privacy-focused setup, and another using OpenRouter."
  },
  {
    "objectID": "posts/aider-with-open-router.html#local-llms",
    "href": "posts/aider-with-open-router.html#local-llms",
    "title": "Use aider for free with your local LLMs or cheaply with OpenRouter",
    "section": "Local LLMs",
    "text": "Local LLMs\n\nMake sure you have ollama installed and running.\nNote down a wich model(s) you have installed and plan to use. We’ll use deepseek-r1 and qwen2.5-coder as example models. Deepseek is general purpose and a good candidate for reasoning while qwen2.5-coder is specialized for coding tasks.\n\n$ ollama list\n\nNAME                                        ID              SIZE      MODIFIED\ndeepseek-r1:14b                             ea35dfe18182    9.0 GB    2 hours ago\nqwen2.5-coder:14b                           9ec8897f747e    9.0 GB    2 hours ago\nI’m using the 14-B distilled models based on my hardware. You can experiment with different ones and find what speed vs quality tradeoff you’re comfortable with. The Ollama models site is very handy to get information about models and their distilled versions.\n\nfollow the guide which tells you to run:\n\naider --model ollama_chat/&lt;model&gt;\nSo in our case, that becomes:\naider --model \"ollama_chat/deepseek-r1:14b\" --editor-model \"ollama_chat/qwen2.5-coder:14b\"\nWe could simply use one model for everything but this “plan vs execution” pattern works really well both locally and remotely.\nUse aider --help or visit the options page on aider’s site to understand the differences between --model (main model), --editor-model (editor tasks), and --weak-model (commit messages and history summarization)."
  },
  {
    "objectID": "posts/aider-with-open-router.html#cheaply-with-openrouter",
    "href": "posts/aider-with-open-router.html#cheaply-with-openrouter",
    "title": "Use aider for free with your local LLMs or cheaply with OpenRouter",
    "section": "Cheaply with OpenRouter",
    "text": "Cheaply with OpenRouter\nIf you’re not satisfied with using your hardware for everything and are ok with sending data to an LLM in the cloud, you can use OpenRouter.\nThe advantage of using OpenRouter over a specific LLM service like Claude, ChatGPT API or others is that you can have a cloud independent approach and mix and match APIs paying in only one place, while also setting specific budgets that you can’t go over.\nuser u/Baldur-Norddahl Reddit LocalLLama shared a snippet of what it looks like. You’ll notice it’s very similar to our local example with the addition of the OpenRouter API Key as an environment variable and that we use Claude 3.7 and the full version of Deepseek r1:\nexport OPENROUTER_API_KEY=sk-or-v1-xxxx\naider --architect --model openrouter/deepseek/deepseek-r1 --editor-model openrouter/anthropic/claude-3.7-sonnet --watch-files\nYou can easily monitor your activity an estimate what your coding sessions are actually like. This may lead you to switch from Claude 3.7 to something cheaper. Again, it’s all about personal experience and quality tradeoffs."
  },
  {
    "objectID": "posts/aider-with-open-router.html#in-closing",
    "href": "posts/aider-with-open-router.html#in-closing",
    "title": "Use aider for free with your local LLMs or cheaply with OpenRouter",
    "section": "In Closing",
    "text": "In Closing\nBoth patterns are very useful and allow you a great degree of flexibility. There’s a lot of power in customization and avoiding vendor lock-in. You’ll be able to experiement with cline/aider or whatever the next tool is. As hardware becomes more powerful, you could have a very productive experience on a plane, even without internet access.\nShoutout to Georgi Gerganov’s llama.cpp which is the core that allows ollama to work."
  },
  {
    "objectID": "posts/meeting-budget.html",
    "href": "posts/meeting-budget.html",
    "title": "Set a Meeting Budget",
    "section": "",
    "text": "Pain Point\n\nToo many recurring meetings drain your week’s productivity.\n\n\nThe Rule\n\nSet a hard budget for fixed meetings.\nExample: 40 h week, 6 h meeting budget.\n\nTotal - Budget = Free -&gt; 40 - 6 = 34\n\nIf you go over budget, cut or shrink the least important meetings.\nYou can adjust the budget, but do so rarely, otherwise it loses meaning.\nAd-hoc syncs are fine. It’s the recurring ones that eat up your time.\n\nConsider doing a similar thing for ad-hoc meetings, if they become a problem.\n\nLike code, less is better. Always look for ways to reduce, even if you’re under budget.\n\n\n\nAnalogy\nThis is similar to the U.S. Senate’s PAYGO rule:\n\nif Congress wants to add $N to a program, they must “find room” by reducing $N somewhere else or by increasing taxes to cover it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Living Deadline",
    "section": "",
    "text": "Measure First, Optimize Last: My Approach to AI Evals\n\n\n\n\n\n\nengineering\n\ngenai\n\nevals\n\n\n\nA pain-driven approach to AI evals: measure first, constrain risk, iterate fast, optimize last.\n\n\n\n\n\nFeb 10, 2026\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nGet the Value of a High-Quality Audit, All the Time\n\n\n\n\n\n\nengineering\n\nproductivity\n\ngenai\n\nops\n\n\n\nTurn pain points into repeatable questions and build a lightweight taxonomy of your world.\n\n\n\n\n\nJan 29, 2026\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nReducing Error Compounding in GenAI Systems\n\n\n\n\n\n\nengineering\n\ngenai\n\nevals\n\n\n\nHow chaining LLM calls compounds errors, and why replacing probabilistic steps with deterministic ones improves reliability.\n\n\n\n\n\nJan 28, 2026\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nStop Reformatting Markdown When Pasting into Slack\n\n\n\n\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nJan 16, 2026\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nFix: pip hangs in WSL (IPv6 / gai.conf)\n\n\n\n\n\n\ntroubleshooting\n\n\n\n\n\n\n\n\n\nJan 13, 2026\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nTalks: Toward a Shared Vision for LLM Evaluation in the Airflow Ecosystem\n\n\nAirflow Summit 2025 - Lightning Talk (5 min)\n\n\n\nairflow\n\ntalks\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Data Build Tool (dbt) to Accelerate & Scale Science\n\n\n\n\n\n\ndata-science\n\nengineering\n\n\n\n\n\n\n\n\n\nSep 1, 2025\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nUse aider for free with your local LLMs or cheaply with OpenRouter\n\n\n\n\n\n\ngenai\n\ncode\n\n\n\n\n\n\n\n\n\nJul 6, 2025\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nMerge and Forget\n\n\n\n\n\n\nengineering\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nNo News Is Good News\n\n\n\n\n\n\nengineering\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nSet a Meeting Budget\n\n\n\n\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nGet notifications in ubuntu when command line tasks end\n\n\n\n\n\n\ndevops\n\n\n\n\n\n\n\n\n\nApr 16, 2019\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\n\n\n\n\n\n\n\nAccept a self-signed certificate with git\n\n\n\n\n\n\ndevops\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nAlex Guglielmone Nemi\n\n\n\n\n\nNo matching items"
  }
]